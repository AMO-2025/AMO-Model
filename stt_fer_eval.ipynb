{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper\n",
        "!pip install transformers accelerate\n",
        "!apt-get update && apt-get install -y ffmpeg"
      ],
      "metadata": {
        "id": "eevxMvBOzMFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "음성 -> text 는 whisper 패키지를 사용한다"
      ],
      "metadata": {
        "id": "wloAP7HNFHwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "def transcribe_audio(audio_path):\n",
        "    result = model.transcribe(audio_path, language=\"ko\")\n",
        "    return result[\"text\"]"
      ],
      "metadata": {
        "id": "CqU_iA_2PgRW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_file_path = \"/content/drive/MyDrive/data/record/imsosory.m4a\"  # wav, mp3, m4a, webm, ogg, flac 가능\n",
        "text_from_audio = transcribe_audio(audio_file_path)\n",
        "print(\"Transcribed Text:\", text_from_audio)"
      ],
      "metadata": {
        "id": "NVwpqW6xFFFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "표정값은 efficientnetb0 모델 + weight 값 inference 해오기"
      ],
      "metadata": {
        "id": "PURTDLz_FW8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import efficientnet_b0\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = efficientnet_b0(pretrained=True)\n",
        "\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(model.classifier[1].in_features, 7)\n",
        ")\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/weight/efficient_fer2013_pretrained.pth', map_location=device))\n",
        "model = model.to(device)\n",
        "emotion_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprised']\n",
        "\n",
        "def final_emotion(image_path, model, transform, device):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        probs = F.softmax(output, dim=1)\n",
        "        pred_idx = torch.argmax(probs, dim=1).item()\n",
        "        pred_label = emotion_labels[pred_idx]\n",
        "        confidence = probs[0][pred_idx].item()\n",
        "\n",
        "    return pred_label, confidence"
      ],
      "metadata": {
        "id": "PRBmfIt-Fb1Q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = '/content/drive/MyDrive/data/pic/IMG_3688.jpg'\n",
        "image_emotion, prob = final_emotion(test_image_path, model, val_transforms, device)\n",
        "print(f\"Predicted emotion: {image_emotion} ({prob*100:.2f}%)\")"
      ],
      "metadata": {
        "id": "Pl4Wiip6PqqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "로컬 llm으로 텍스트랑 표정 일치 평가 진행"
      ],
      "metadata": {
        "id": "ifFUB8JtFNcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "client = Groq(api_key=api_key)"
      ],
      "metadata": {
        "id": "TIGr2vJzMa1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scenario = \"\"\"\n",
        "엄마: 은우야, 밥 먹으러 오렴.\n",
        "은우: 네 알겠어요.\n",
        "그렇지만 은우는 계속 장난감을 가지고 놀았어.\n",
        "엄마: 은우야, 그만 놀고 밥 먹으러 나오렴.\n",
        "은우: 네 알겠어요.\n",
        "그치만 장난감을 가지고 노는 게 너무 재밌는걸.\n",
        "엄마: 은우야, 밥 먹으러 오라고 3번째 말하고 있어.\n",
        "엄마의 목소리는 조금 큰 것 같아.\n",
        "이때 은우는 어떻게 답해야 할까?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MyQGFg0aOXBp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_alignment(text, image_emotion):\n",
        "    prompt = f\"\"\"\n",
        "You are an evaluator of emotional appropriateness in context.\n",
        "\n",
        "Scenario:\n",
        "\"{scenario}\"\n",
        "\n",
        "User said (via speech): \"{text_from_audio}\"\n",
        "Their facial expression was: \"{image_emotion}\"\n",
        "\n",
        "Step 1: Identify the emotion implied in the speech. Use Ekman's 6 basic emotions: happy, sad, angry, fear, surprise, disgust.\n",
        "\n",
        "Step 2: Evaluate the emotional alignment between the speech and facial expression using:\n",
        "1. Emotion Label Match (same emotion)\n",
        "2. Valence Match (positive/negative)\n",
        "3. Arousal Match (low/medium/high activation)\n",
        "4. Contextual Fit (appropriate emotion for the scenario)\n",
        "\n",
        "Score each from 0 to 25. Total is 100.\n",
        "\n",
        "Only return:\n",
        "LabelScore, ValenceScore, ArousalScore, ContextScore, TotalScore, InferredSpeechEmotion, Reason (one sentence).\n",
        "\n",
        "Important instructions:\n",
        "- Only output raw scores and a short reason.\n",
        "- DO NOT restate the inputs.\n",
        "- Format:\n",
        "LabelScore: [0-25]\n",
        "ValenceScore: [0-25]\n",
        "ArousalScore: [0-25]\n",
        "ContextScore: [0-25]\n",
        "TotalScore: [0-100]\n",
        "Reason: [short sentence]\n",
        "\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",  # 아니면 llama3-70b-8192 쓰기\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "jXB0v1fDMZeC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval = evaluate_alignment(text_from_audio, image_emotion)\n",
        "print(eval)"
      ],
      "metadata": {
        "id": "bvfG5DCVMcIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}